\documentclass[20pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{emory-theme}
\usepackage[scaled]{helvet}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{wrapfig}
\usetikzlibrary{shapes.geometric,calc}


\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}

% Set theme parameters,
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

% Implement table environment, since this is not part of the 'tikzposter' package.
% For reference see: 
% https://tex.stackexchange.com/questions/337953/tikzposter-table-environment
\makeatletter
\newcounter{tablecounter}
\newenvironment{tikztable}[1][]{
	\def \rememberparameter{#1}
	\vspace{10pt}
	\refstepcounter{tablecounter}
	\begin{center}
	}{
		\ifx\rememberparameter\@empty
		\else
		\\[10pt]
		{\small Tab.~\thetablecounter: \rememberparameter}
		\fi
	\end{center}
}
\makeatother

% Header
\title{Parallelization in Multiple Imputation}
\author{Sven Nekula, Joshua Simon and Eva Wolf}
\institute{Otto Friedrich University, Bamberg}
\titlegraphic{\includegraphics[width=0.06\textwidth]{img/uni_ba_logo_100_blau.png}}

\newcommand\score[2]{%
	\pgfmathsetmacro\pgfxa{#1 + 1}%
	\tikzstyle{scorestars}=[star, star points=5, star point ratio=5, draw, inner sep=1.3pt, anchor=outer point 3]%
	\begin{tikzpicture}[baseline]
		\foreach \i in {1, ..., #2} {
			\pgfmathparse{\i<=#1 ? "yellow" : "gray"}
			\edef\starcolor{\pgfmathresult}
			\draw (\i*1.75ex, 0) node[name=star\i, scorestars, fill=\starcolor]  {};
		}
	\end{tikzpicture}%
}

% begin document
\renewcommand\familydefault{\sfdefault}
\begin{document}
\maketitle
\centering
\begin{columns}
    \column{0.32}
    \block{What is Parallelization?}{
    	\begin{wrapfigure}{l}{5cm}
    		\centering
    	    \includegraphics[width=5cm]{img/cpu.png}
    	    \caption{4 processors with 4 cores each}
    	\end{wrapfigure}
         \textcolor{earthyellow}{\textbf{Parallelization}} is a technique to fasten time-consuming computations. It uses several  cores on a \textbf{CPU} parallely and splits up the computational work on them. Afterwords, the results are merged. This can reduce the runtime significantly. Parallelization is nevertheless limited to a certain kind of tasks: The computations to be parallelized must be independent from each other, as information cannot be cross-accessed over the cores while the process is running.\\
    }

	\block{Theory}{
		\begin{wrapfigure}{r}{15cm}
			\centering
			\includegraphics[width=15cm]{img/600px-AmdahlsLaw.svg}
			\caption{Amdahls Law}
		\end{wrapfigure}
		In \textcolor{earthyellow}{\textbf{parallel programming}}, the multiple cores of a computational system want to be used best to decrease computation time. Gene \textbf{Amdahl} was the first one to desribe the boundaries of that project: Every parallel process also requires additional workload, so called "data management housekeeping". The speed up through parallel processing will tend to 0 at a certain amount of processing units involved, as this \textbf{overhead} workload exceeds the capacity of the computational unit which it is assigned to.\\
		\textcolor{earthyellow}{\textbf{Multiple imputation}} is a method to complete a dataset with missing information. It relies on the estimation of the missing values through different methods. What is common to them is that we use not a single imputation run, but several. The results of all imputation runs are then merged and lead to realistic uncertainty of the estimators given the missing data. It is prone to be parallelized, as the several imputation runs can be processed independend of each other on different cores and easily merged afterwards. 
	}
	\block{Methodology}{
	As the \textcolor{earthyellow}{\textbf{dataset}} we used a simple data generator of normally distributed random variables. The data set created contains 10 variables, of which some depend on each other. The sample size is n=10000. Parallelizationfavors complex data sets, so the amount of variables and their interdependency should not be to small.\\
	\textcolor{earthyellow}{\textbf{Time measurement}} was done with the \textit{system.time} function, which returns 3 values: User CPU-, User System- and Elapsed time. User CPU is the time needed for the current task such as an execution in R. System CPU describes the time needed by the operating system to organize that task such as opening folders or asking for the System time. Elapsed time is the wall clock time that passed while the function and background processes were running.\\
	The \textcolor{earthyellow}{\textbf{method}} of the \textcolor{earthyellow}{\textbf{mice}} algorithm was set on defaut, pmm.\\
	The \textcolor{earthyellow}{\textbf{speed up}} value is calculated by the serial time (runtime without parallelization) divided by the runtime of the current parallelization implementation \cite{cite:chapple2016mastering}. Values below one show that the parallelized run took longer than the serial run.\\
	}


    \column{0.36}
    \block{Results}{
        
        \vspace{1em}
			\begin{tikzfigure}[Runtime and Speedup from 1 up to 8 cores]
				\includegraphics[width=1\linewidth]{img/runtime_ncores_speedup}
			\end{tikzfigure}
        \vspace{1em}
       
       	\vspace{1em}
			\begin{tikzfigure}[Runtime and Speedup from 1 up to 128 Imputation runs]
				\includegraphics[width=1\linewidth]{img/runtime_nimp_speedup_resized}
			\end{tikzfigure}
       	\vspace{1em}
       
      	\vspace{1em}
			%\begin{tikzfigure}[Time components for the different implementations on 16 cores]
			%	\includegraphics[width=0.7\linewidth]{img/2022-01-15_benchmark_core_Linux_cpu_time}
			%\end{tikzfigure}

			\begin{tikztable}[Time components for the different implementations on 16 cores]
				%\small		% reduce font size within the table
				\begin{tabular}{rlrrrrr}
					\hline
					& Function & Cores & Avg. user time & Avg. system time & Avg. elapsed time & Avg. speed up \\ 
					\hline
					1 & foreach & 16 & 0.25 & 0.03 & 7.63 & 4.48 \\ 
					2 & foreach\_fork & 16 & 0.21 & 0.17 & 7.03 & 5.15 \\ 
					3 & furrr & 16 & 1.07 & 0.09 & 8.93 & 3.83 \\ 
					4 & mice.par & 16 & 1.15 & 0.10 & 13.67 & 2.50 \\ 
					5 & parLapply & 16 & 0.33 & 0.04 & 7.80 & 4.39 \\ 
					6 & parLapply\_fork & 16 & 0.23 & 0.17 & 6.40 & 5.36 \\ 
					7 & parlmice & 16 & 0.30 & 0.02 & 7.76 & 4.41 \\ 
					8 & parlmice\_fork & 16 & 0.21 & 0.18 & 6.54 & 5.25 \\ 
					9 & serial & 16 & 34.11 & 0.03 & 34.13 & 1.00 \\ 
					\hline
				\end{tabular}
			\end{tikztable}
    	\vspace{1em} 
    }


    \column{0.32}
    \block{Comparison of Implementations}{
    	
    	\textbf{foreach::foreach} usability 
		%\hspace{-1mm}\big{ \score{2.4}{5}} \hspace{1cm} runtime \hspace{-1mm}\big{ \score{2.4}{5}} \\
    	\\
    	\textbf{mice::parlmice} usability
    	%\hspace{-1mm} \big{\score{2.4}{5}} %\\
    	\\
    	\textbf{micemd::mice.par} usability
    	%\hspace{-1mm}\big{ \score{2.4}{5}} \\
    	\\
    	\textbf{parallel::parLapply} usability
    	%\hspace{-1mm} \big{ \score{2.4}{5}} \\
    	\\
    	\textbf{furrr::future\_map} usability
    	%\hspace{-1mm}\big{ \score{2.4}{5}} \\ 
    	\\
    	\textbf{future.apply::future\_lapply} usability
    	%\hspace{-1mm} \big{\score{2.4}{5}} \\ 
    	\\
    	\centering{\textit{PSOCK} vs. \textit{FORK}}
    }

    \block{Further Applications}{
    \begin{wrapfigure}{r}{6cm}
    	\includegraphics[width=5cm]{img/disk.framing}
    \end{wrapfigure}
	The opposite case to paralellized processes is the intended time-intensive, serial processing of Big Data with \textcolor{earthyellow}{\textbf{disk framing}}. In that setting we have data that exceeds the \textbf{RAM capacity} of the system, so that the data needs to a split up into several smaller chunks. These are then processed in portions that fit the available memory. Nevertheless, also for \textbf{disk framing}, parallelization is a useful tool to decrease computation time. The system chooses as many chunks of data as they fit into the RAM and then processes them parallely, so that use of the computational power of the system can be made, while the memory capacity limits the speed by the amount of data which is processed at once.\\

	Considering today's trends in parallel and high performance computing, \textcolor{earthyellow}{\textbf{scalability}} plays a big role. So-called distributed systems are used here. These can be seen as the fusion of several computers into one system, often referred to as a \textbf{computing cluster}. In order to run a computer program on such an architecture, a \textbf{message passing interface (MPI)} is used, which now handles the communication between multiple CPUs and their cores. Popular applications are cloud solutions like Amazon's AWS (Amazon Web Services). The R language can make use of MPI with packages like \textbf{Rmpi} or \textbf{pbdMPI} \cite{cite:chapple2016mastering} and together with the \textbf{OpenMPI} implementation utilizing the lower level of parallelism \cite{cite:robey2021parallel}.
	
	}

    \block{References}{
        \vspace{-1em}
        \begin{footnotesize}
        
        \printbibliography
        
        \end{footnotesize}
    }

\end{columns}
\end{document}